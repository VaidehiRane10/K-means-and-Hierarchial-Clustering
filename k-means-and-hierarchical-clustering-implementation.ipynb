{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# K-Means and Hierarchical Clustering Implementation\n\n* Clustering algorithms is being used for unlabelled datasets.\n* This is an implementation example of clustering algorithms.  We'll use K-Means an Hierarchical clustering algorithms for seperate the cancer data by \"radius_mean\" and \"texture_mean\"\n\n## Index of contents\n\n* [DATA EXPLORATION](#1)\n* [K-MEANS CLUSTERING](#2)\n* [HIERARCHICAL CLUSTERING](#3)","metadata":{"_uuid":"6aa5549c35c4e6b147af67029ef94e1d93f92499"}},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"1\"></a> \n**DATA EXPLORATION**","metadata":{"_uuid":"9ca3fef527c201c3534fd3b3ca8c351768e80699"}},{"cell_type":"code","source":"# Read and upload data\ndata = pd.read_csv(\"../input/data.csv\")","metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# We don't need id and NaN data.\ndata.drop([\"Unnamed: 32\", \"id\"], axis = 1, inplace = True)\ndata.head()","metadata":{"_uuid":"6a3e8070caa88d2a7c15ab9fb9c5ad217315828c","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data[\"diagnosis\"].value_counts()\n\n# We have 357 B and 212 M labelled data","metadata":{"_uuid":"e8ac256727a30bff2822a28bd36bb27ed60511c3","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# For clustering we do not need labels. Because we'll identify the labels.\n\ndataWithoutLabels = data.drop([\"diagnosis\"], axis = 1)\ndataWithoutLabels.head()","metadata":{"_uuid":"1fa48cefd9a30de739da2033636b1801d2001b31","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataWithoutLabels.info()","metadata":{"_uuid":"d2c41332e992d5610b2b9045afbd6285e5105fe9","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# radius_mean and texture_mean features will be used for clustering. Before clustering process let's check  how our data looks.\n\nsns.pairplot(data.loc[:,['radius_mean','texture_mean', 'diagnosis']], hue = \"diagnosis\", height = 5)\nplt.show()","metadata":{"_uuid":"e021efc6efc48fdbc0f80c1f083388b7719910a9","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Our data looks like below plot without diagnosis label\n\nplt.figure(figsize = (10, 10))\nplt.scatter(dataWithoutLabels[\"radius_mean\"], dataWithoutLabels[\"texture_mean\"])\nplt.xlabel('radius_mean')\nplt.ylabel('texture_mean')\nplt.show()","metadata":{"_uuid":"c0f458f5d575a5dbddb605674cd6fd5a23dd479e","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"2\"></a> \n**K-MEANS CLUSTERING**\n* Define **K** centers and cluster data,\n* Assign random centroids,\n* Cluster data points according to distance from centroids (euclidean distance),\n* Repeat step 3 until centroid positions start not to change.\n\n![KMeansClustering.png](http://i67.tinypic.com/sdpueu.png)\n* WCSS is a metric used for k value selection process. After this operation elbow rule is used for k value.\n\n![KMeansElbow.png](http://i64.tinypic.com/2dw7ztg.png)","metadata":{"_uuid":"d318aff4fe4d91f11b1597bb9d1f1e24d03a1299"}},{"cell_type":"code","source":"from sklearn.cluster import KMeans\nwcss = [] # within cluster sum of squares\n\nfor k in range(1, 15):\n    kmeansForLoop = KMeans(n_clusters = k)\n    kmeansForLoop.fit(dataWithoutLabels)\n    wcss.append(kmeansForLoop.inertia_)\n\nplt.figure(figsize = (10, 10))\nplt.plot(range(1, 15), wcss)\nplt.xlabel(\"K value\")\nplt.ylabel(\"WCSS\")\nplt.show()","metadata":{"_uuid":"0b6fea5d47a280cd8358b24d2f911eb4ca9cdddd","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Elbow point starting from 2 \n\ndataWithoutLabels = data.loc[:,['radius_mean','texture_mean']]\nkmeans = KMeans(n_clusters = 2)\nclusters = kmeans.fit_predict(dataWithoutLabels)\ndataWithoutLabels[\"type\"] = clusters\ndataWithoutLabels[\"type\"].unique()","metadata":{"_uuid":"cea037fd22671b7922504a9783b6c07eb6ed4a38","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Plot data after k = 2 clustering\n\nplt.figure(figsize = (15, 10))\nplt.scatter(dataWithoutLabels[\"radius_mean\"][dataWithoutLabels[\"type\"] == 0], dataWithoutLabels[\"texture_mean\"][dataWithoutLabels[\"type\"] == 0], color = \"red\")\nplt.scatter(dataWithoutLabels[\"radius_mean\"][dataWithoutLabels[\"type\"] == 1], dataWithoutLabels[\"texture_mean\"][dataWithoutLabels[\"type\"] == 1], color = \"green\")\nplt.xlabel('radius_mean')\nplt.ylabel('texture_mean')\nplt.show()","metadata":{"_uuid":"853f58f574228f7400126c4003ffc948ee6c20c6","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Data centroids middle of clustered scatters\n\nplt.figure(figsize = (15, 10))\nplt.scatter(dataWithoutLabels[\"radius_mean\"], dataWithoutLabels[\"texture_mean\"], c = clusters, alpha = 0.5)\nplt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1], color = \"red\", alpha = 1)\nplt.xlabel('radius_mean')\nplt.ylabel('texture_mean')\nplt.show()","metadata":{"_uuid":"ba41667cb35f143bf6da40d65057c3138782ca86","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataWithoutDiagnosis = data.drop([\"diagnosis\"], axis = 1)\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.pipeline import make_pipeline\nscalar = StandardScaler()\nkmeans = KMeans(n_clusters = 2)\npipe = make_pipeline(scalar, kmeans)\npipe.fit(dataWithoutDiagnosis)\nlabels = pipe.predict(dataWithoutDiagnosis)\ndf = pd.DataFrame({'labels': labels, \"diagnosis\" : data['diagnosis']})\nct = pd.crosstab(df['labels'], df['diagnosis'])\nprint(ct)","metadata":{"_uuid":"0d8bb11d73e68a6fffe9896773fbc6ca4670ad5f","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"3\"></a> \n**HIERARCHICAL CLUSTERING**\n* Each data point is transformed to cluster,\n* Create cluster using closest 2 data point,\n* Create cluster using closest 2 cluster,\n* Repeat step 3.\n\n![dendogram.PNG](http://i66.tinypic.com/97piew.png)\n\n* Above mentioned distances are euclidean distances\n* Dendogram is used for n_clusters value detection.","metadata":{"_uuid":"86f284a56bf45b959e387fcd64888e1c42658447","trusted":true}},{"cell_type":"code","source":"dataWithoutTypes = dataWithoutLabels.drop([\"type\"], axis = 1)\ndataWithoutTypes.head()","metadata":{"_uuid":"6354bf2777aceecc9092d4ca24e7358780f986a1","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from scipy.cluster.hierarchy import linkage,dendrogram\nmerg = linkage(dataWithoutTypes, method = \"ward\")\ndendrogram(merg, leaf_rotation = 90)\nplt.xlabel(\"data points\")\nplt.ylabel(\"euclidean distance\")\nplt.show()\n","metadata":{"_uuid":"33346e0cf8f05134a62ee79c3e130c6dd09bc703","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.cluster import AgglomerativeClustering\nhc = AgglomerativeClustering(n_clusters = 2, affinity = \"euclidean\", linkage = \"ward\")\ncluster = hc.fit_predict(dataWithoutTypes)\ndataWithoutTypes[\"label\"] = cluster","metadata":{"_uuid":"39d0ae62a732ed366e98522f4f1df6e1550d95cf","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataWithoutTypes.label.value_counts()","metadata":{"_uuid":"43f5128919b4b08facf4c34a2148ffec900f85ee","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Data after hierarchical clustering\n\nplt.figure(figsize = (15, 10))\nplt.scatter(dataWithoutTypes[\"radius_mean\"][dataWithoutTypes.label == 0], dataWithoutTypes[\"texture_mean\"][dataWithoutTypes.label == 0], color = \"red\")\nplt.scatter(dataWithoutTypes[\"radius_mean\"][dataWithoutTypes.label == 1], dataWithoutTypes[\"texture_mean\"][dataWithoutTypes.label == 1], color = \"blue\")\nplt.xlabel(\"radius_mean\")\nplt.ylabel(\"texture_mean\")\nplt.show()","metadata":{"_uuid":"c390c20b9dd399f6189cffed105dde6e5ea9c5d4","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Conclusion","metadata":{"_uuid":"fb1c12968acf5f6e07c648f532d494fa2f5533ee","trusted":true}},{"cell_type":"markdown","source":"*We used unsupervised learning algorithms for clustering cancer data. Thank you for reading my kernel. Please do not hesitate to leave comments.*","metadata":{"_uuid":"c819f212adfbaceb61d569f0e3d57fb993951abc","trusted":true}}]}